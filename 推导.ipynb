{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求解法\n",
    "在这一部分，我们将详细讲解上个视频中提到的一些概念。\n",
    "\n",
    "**图片**\n",
    "\n",
    "关于贝尔曼期望方程的注释\n",
    "在上个视频中，我们为每个环境状态推导了一个方程。例如，对于状态 $s_1$，我们发现：\n",
    "$v_\\pi(s_1) = \\frac{1}{2}(-1 + v_\\pi(s_2)) + \\frac{1}{2}(-3 + v_\\pi(s_3))$\n",
    "\n",
    "我们提到，该方程直接来自 $v_\\pi$的贝尔曼期望方程。\n",
    "\n",
    "我们提到，该方程直接来自 $v_\\pi$的贝尔曼期望方程。\n",
    "\n",
    ">$v_\\pi(s) = \\text{} \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t=s] = \\sum_{a \\in \\mathcal{A}(s)}\\pi(a|s)\\sum_{s' \\in \\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s,a)(r + \\gamma v_\\pi(s'))$ 的**贝尔曼期望方程**）**Bellman Expectation Equation for the State-Value Function**\n",
    "\n",
    "为了帮助理解，我们先看看根据贝尔曼期望方程，状态 $s_1$的值是多少（我们只需代入 s1 替换公式中的 s ）。\n",
    "\n",
    "$v_\\pi(s_1) = \\sum_{a \\in \\mathcal{A}(s_1)}\\pi(a|s_1)\\sum_{s' \\in \\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s_1,a)(r + \\gamma v_\\pi(s'))$\n",
    "\n",
    "然后根据以下公式得出状态 $s_1$的方程：\n",
    "\n",
    "- $\\mathcal{A}(s_1)=\\{ \\text{down}, \\text{right} \\}$(在状态 $s_1$时，智能体只能执行两个潜在动作：向下或向右。）\n",
    "- $\\pi({down}|s_1) = \\pi(\\text{right}|s_1) = \\frac{1}{2}$（我们目前研究的策略是：智能体在状态 $s_1$时向下移动的概率是 50%，向右移动的概率是 50%。）\n",
    "- $p(s_3,-3|s_1,\\text{down}) = 1$（以及$ p(s',r|s_1,\\text{down}) = 0$，前提是 $s'\\neq s_3$或 $r\\neq -3$）（如果智能体在状态 $s_1$时选择向下移动，那么下个状态 100% 是 $s_3$，智能体获得奖励 -3。）\n",
    "- $p(s_2,-1|s_1,\\text{right}) = 1$（以及 $p(s',r|s_1,\\text{right}) = 0$，前提是 $s'\\neq s_2$或 $r\\neq -1$）（如果智能体在状态 s_1时选择向右移动，那么下个状态 100% 是 $s_2$，智能体获得奖励 -1。）\n",
    "- $\\gamma = 1$（在这个网格世界示例中，我们选择将折扣率设为 1。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于求解方程组的注释\n",
    "在视频中，我们提到你可以直接求解方程组：\n",
    "\n",
    "$v_\\pi(s_1) = \\frac{1}{2}(-1 + v_\\pi(s_2)) + \\frac{1}{2}(-3 + v_\\pi(s_3))$\n",
    "\n",
    "$v_\\pi(s_2) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + v_\\pi(s_4))$\n",
    "\n",
    "$v_\\pi(s_3) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + v_\\pi(s_4))$\n",
    "\n",
    "$v_\\pi(s_4) = 0$\n",
    "\n",
    "因为 $v_\\pi(s_2)$ 和 $v_\\pi(s_3)$的方程一样，因此我们必须确保 $v_\\pi(s_2) = v_\\pi(s_3)$\n",
    "因此，$v_\\pi(s_1)$和 $v_\\pi(s_2)$的方程可以更改为：\n",
    "\n",
    "$v_\\pi(s_1) = \\frac{1}{2}(-1 + v_\\pi(s_2)) + \\frac{1}{2}(-3 + v_\\pi(s_2)) = -2 + v_\\pi(s_2)$\n",
    "\n",
    "$v_\\pi(s_2) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + 0) = 2 + \\frac{1}{2}v_\\pi(s_1)$\n",
    "\n",
    "将最新的两个方程相结合，生成\n",
    "\n",
    "$v_\\pi(s_1) = -2 + 2 + \\frac{1}{2}v_\\pi(s_1) = \\frac{1}{2}v_\\pi(s_1)$\n",
    "\n",
    "表明 $v_\\pi(s_1)=0$。此外，\n",
    "$v_\\pi(s_3) = v_\\pi(s_2) = 2 + \\frac{1}{2}v_\\pi(s_1) = 2 + 0 = 2$。\n",
    "\n",
    "因此，状态值函数可以通过以下方程组获得：\n",
    "\n",
    "$v_\\pi(s_1) = 0$\n",
    "\n",
    "$v_\\pi(s_2) = 2$\n",
    "\n",
    "$v_\\pi(s_3) = 2$\n",
    "\n",
    "$v_\\pi(s_4) = 0$\n",
    "\n",
    "注意。此示例表明我们_可以__直接求解 $v_\\pi$\n",
    " 的贝尔曼期望方程给出的方程组。但是，在现实中，尤其是对于大型马尔可夫决策流程 (MDP) 来说，我们将改用迭代_方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迭代法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bellman Expectation Equation for the State-Value Function**\n",
    ">$v_\\pi(s) = \\text{} \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t=s] = \\sum_{a \\in \\mathcal{A}(s)}\\pi(a|s)\\sum_{s' \\in \\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s,a)(r + \\gamma v_\\pi(s'))$ \n",
    "\n",
    "**Update Rule for Iterative Police Evaluation**\n",
    ">$V_\\pi(s) = \\text{} \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t=s] = \\sum_{a \\in \\mathcal{A}(s)}\\pi(a|s)\\sum_{s' \\in \\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s,a)(r + \\gamma V_\\pi(s'))$\n",
    "\n",
    "$V$是对police value-$v_{\\pi}$的猜测\n",
    "\n",
    "### **实现：迭代策略评估**\n",
    "\n",
    "<div align=center><img width=\"550\" src=resource/3.png></div>\n",
    "\n",
    "注意：只要对于每个状态$ s\\in\\mathcal{S}s∈S，v_\\pi(s)$是有限的，策略评估就保证会收敛于策略 $\\pi$ 对应的状态值函数。对于有限的马尔可夫决策流程 (MDP)，只要满足以下条件之一，就保证会收敛：\n",
    "\n",
    "- $\\gamma < 1$，或\n",
    "- 如果智能体以任何状态 $s\\in\\mathcal{S}$ 开始，并且遵守 $\\pi$，就保证会最终达到终止状态。\n",
    "\n",
    "在动态规划设置中，智能体完全了解马尔可夫决策流程 (MDP)。在这种情况下，可以使用 MDP 的一步动态 p(s',r|s,a)获得 $v_\\pi$的贝尔曼期望方程对应的方程组。\n",
    "\n",
    "在网格世界示例中，等概率随机策略对应的方程组由以下方程获得：\n",
    "\n",
    "$v_\\pi(s_1) = \\frac{1}{2}(-1 + v_\\pi(s_2)) + \\frac{1}{2}(-3 + v_\\pi(s_3))$\n",
    "\n",
    "$v_\\pi(s_2) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + v_\\pi(s_4))$ \n",
    "\n",
    "$v_\\pi(s_3) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + v_\\pi(s_4))$ \n",
    "\n",
    "$v_\\pi(s_4) = 0$\n",
    "\n",
    "为了获得状态值函数，我们只需求解该方程组。\n",
    "\n",
    "虽然也许始终可以直接求解方程组，但是我们将改用迭代方法。\n",
    "\n",
    "迭代方法先对每个状态的值进行初始猜测。尤其是，我们先假设每个状态的值为 0。\n",
    "\n",
    "然后，循环访问状态空间并通过应用连续的更新方程修改状态值函数的估算结果。\n",
    "\n",
    "注意，V 表示状态值函数的最新猜测，更新方程为：\n",
    "\n",
    "$V(s_1) \\leftarrow \\frac{1}{2}(-1 + V(s_2)) + \\frac{1}{2}(-3 + V(s_3))$ \n",
    "\n",
    "$V(s_2) \\leftarrow \\frac{1}{2}(-1 + V(s_1)) + \\frac{1}{2}(5)$\n",
    "\n",
    "$V(s_3) \\leftarrow \\frac{1}{2}(-1 + V(s_1)) + \\frac{1}{2}(5)$\n",
    "\n",
    "\n",
    "为了直观地了解收敛条件_为何_很合理，思考下两个条件都不满足的情形，因此：\n",
    "\n",
    "- $\\gamma = 1$，以及\n",
    "- 具有状态 $s\\in\\mathcal{S}$，如果智能体从该状态开始，则遵守策略 $\\pi$ 的话，它将始终不会遇到终止状态。\n",
    "\n",
    "在这种情况下，\n",
    "\n",
    "- 奖励没有折扣\n",
    "- 某个阶段可能永远不会结束。\n",
    "\n",
    "那么迭代策略评估可能不会收敛，这是因为状态值函数可能定义不合理！为此，注意在此情形下，计算状态值可能需要将无穷多的（预期）奖励相加，和可能不会收敛。\n",
    "\n",
    "我们来举一个具体的例子，假设某个 MDP：\n",
    "\n",
    "- 具有两个状态 $s_1$ 和 $s_2$，其中 $s_2$是终止状态\n",
    "- 具有一个动作 a（_注意：只有一个动作的 MDP 还可以称之为[马尔可夫奖励流程 (MRP)](https://en.wikipedia.org/wiki/Markov_reward_model)。_）\n",
    "- p(s_1,1|s_1, a) = 1\n",
    "在这种情况下，假设智能体的策略 $\\pi$ 是仅“选择”可以执行的动作，因此 $\\pi(s_1) = a$。假设 $\\gamma = 1$。根据一步动态特性，如果智能体从状态 $s_1$开始，它将始终保持该状态，永远不会遇到终止状态 s_2s \n",
    "。\n",
    "\n",
    "在这种情况下，$v_\\pi(s_1)$ 定义不合理。为此，注意 $v_\\pi(s_1)$是经历状态$ s_1$ 之后的（预期）回报，并且\n",
    "\n",
    "$v_\\pi(s_1) = 1 + 1 + 1 + 1 + ...$\n",
    "\n",
    "发散为正无穷。（请花时间消化这段内容，并明白如果在此示例中，其中一个条件满足了，那么 $v_\\pi(s_1)$将定义合理。作为可选下一步，如果你想从数学角度满足这一条件，建议你复习几何级数https://en.wikipedia.org/wiki/Geometric_series 和 \n",
    "负二项分布https://en.wikipedia.org/wiki/Negative_binomial_distribution。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动作值\n",
    "在上一部分，你自己实现了迭代策略评估，以估算策略 $\\pi$ 的状态值函数 $v_\\pi$。在此部分，你将使用视频中的简单网格世界练习将状态值函数 $v_\\pi$转换为动作值函数 $q_\\pi$。\n",
    "\n",
    "思考下我们用来表示迭代策略评估的小网格世界。等概率随机策略的状态值函数可视化结果如下。\n",
    "\n",
    "\n",
    "<div align=center><img width=\"550\" src=resource/1.png></div>\n",
    "现在花时间确认以下图片对应的是同一策略的动作值函数。\n",
    "<div align=center><img width=\"550\" src=resource/2.png></div>\n",
    "\n",
    "思考下 $q_\\pi(s_1, \\text{right})$这个示例。这个动作值的计算方式如下所示：\n",
    "\n",
    "$q_\\pi(s_1, \\text{right}) = -1 + v_\\pi(s_2) = -1 + 2 = 1$\n",
    "\n",
    "我们可以将状态动作对的值 $s_1, \\text{right}$表示为以下两个量的和：(1)向右移动并进入状态 $s_2$的即时奖励，以及 (2) 智能体从状态 $s_2$开始并遵守该策略获得的累积奖励。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 对于更加复杂的环境**\n",
    "\n",
    "在这个简单的网格世界示例中，环境是确定性环境。换句话说，智能体选择某个动作后，下个状态和奖励可以 100% 确定不是随机的。对于确定性环境，所有的 $s', r, s, a为 p(s',r|s,a) \\in \\{ 0,1 \\}$\n",
    "\n",
    ">在这种情况下，当智能体处在状态 s 并采取动作 a 时，下个状态 s' 和奖励 r可以确切地预测，我们必须确保 $q_\\pi(s,a) = r + \\gamma v_\\pi(s')$。\n",
    "\n",
    "通常，环境并非必须是确定性环境，可以是随机性的。这是迷你项目中的 FrozenLake 环境的默认行为；\n",
    ">在这种情况下，智能体选择动作后，下个状态和奖励无法确切地预测，而是从（条件性）概率分布 p(s',r|s,a)中随机抽取的。\n",
    "\n",
    "在这种情况下，当智能体处在状态s并采取动作a时，每个潜在下个状态 s'的概率和奖励 r 由 p(s',r|s,a)定。在这种情况下，我们必须确保$ q_\\pi(s,a) = \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,a)(r+\\gamma v_\\pi(s'))$，我们计算和$ r + \\gamma v_\\pi(s')$的期望值。\n",
    "\n",
    "在接下来的几个部分，你将使用该方程为 FrozenLake 环境编写一个函数，并生成策略 $\\pi$ 对应的动作值函数 $q_\\pi$ 。\n",
    "\n",
    "<div align=center><img width=\"550\" src=resource/2.png></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**---------------------------------------------------------------------------------------**\n",
    "简介\n",
    "\n",
    "- 在动态规划设置中，智能体完全了解 MDP（这比强化学习设置简单多了。在强化学习设置中，智能体一开始完全不了解环境如何确定状态和动作，必须完全通过互动来了解如何选择动作。）\n",
    "\n",
    "迭代方法\n",
    "\n",
    "- 为了获得策略 \\piπ 对应的状态值函数 $v_\\pi$，我们只需求解 $v_\\pi$的贝尔曼预期方程对应的方程组。\n",
    "虽然可以通过分析方式求解方程组，但是我们将重点讲解迭代方法。\n",
    "\n",
    "### **实现：迭代策略评估**\n",
    "- 迭代策略评估是在动态规划设置中用到的算法，用于估算策略 $\\pi$ 对应的状态值函数 $v_\\pi$。在此方法中，我们将向值函数估值中应用贝尔曼更新，直到估值的变化几乎不易觉察\n",
    "\n",
    "<div align=center><img width=\"550\" src=resource/3.png></div>\n",
    "\n",
    "注意：只要对于每个状态$ s\\in\\mathcal{S}s∈S，v_\\pi(s)$是有限的，策略评估就保证会收敛于策略 $\\pi$ 对应的状态值函数。对于有限的马尔可夫决策流程 (MDP)，只要满足以下条件之一，就保证会收敛：\n",
    "\n",
    "- $\\gamma < 1$，或\n",
    "- 如果智能体以任何状态 $s\\in\\mathcal{S}$ 开始，并且遵守 $\\pi$，就保证会最终达到终止状态。\n",
    "\n",
    "\n",
    "### **实现：动作值的估值**\n",
    "在动态规划设置中，可以使用以下方程从状态值函数 $v_\\pi$快速获得动作值函数 $q_\\pi$：$q_\\pi(s,a) = \\sum_{s'\\in\\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s,a)(r+\\gamma v_\\pi(s'))$。\n",
    "\n",
    "在下个部分，你将编写一个算法，该算法会接受状态值函数 $v_\\pi$的估值 V，以及 MDP $p(s',r|s,a)$的一步动态特性并返回动作值函数 $q_\\pi$的估值 $Q$。\n",
    "\n",
    "为此，你需要使用在上一部分讨论的方程，该方程使用马尔可夫决策流程 (MDP) 的一步动态特性 $p(s',r|s,a)$获得来自 $v_\\pi$的 $q_\\pi$，即\n",
    "\n",
    "$q_\\pi(s,a) = \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,a)(r+\\gamma v_\\pi(s'))$\n",
    "\n",
    "针对所有 $s\\in\\mathcal{S}$ 和 $a\\in\\mathcal{A}(s)$。\n",
    "\n",
    "你可以在下方找到相关的伪代码\n",
    "<div align=center><img width=\"550\" src=resource/4.png></div>\n",
    "\n",
    "### **实现：策略改进**\n",
    "- 策略改进对策略 $\\pi$ 对应的动作值函数 $v_\\pi$进行估算 V，并返回改进（或对等）的策略 $\\pi'$，其中 $\\pi'\\geq\\pi$。该算法首先构建动作值函数估值 Q。然后，对应每个状态 $s\\in\\mathcal{S}$，你只需选择最大化 Q(s,a) 的动作 a。换句话说，$\\pi'(s) = \\arg\\max_{a\\in\\mathcal{A}(s)}Q(s,a)$，针对所有 $s\\in\\mathcal{S}$。\n",
    "\n",
    "在上一节课，你学到了以下知识：如果给定策略 $\\pi$ 对应的动作值函数 $q_\\pi$的估值 Q，可以构建一个改进（或对等）的策略 $\\pi'$，其中 $\\pi'\\geq\\pi$。\n",
    "\n",
    "对于每个状态 $s\\in\\mathcal{S}$，你只需选择最大化动作值函数估值的动作。换句话说，\n",
    "\n",
    "$\\pi'(s) = \\arg\\max_{a\\in\\mathcal{A}(s)}Q(s,a)$，针对所有 $s\\in\\mathcal{S}$。\n",
    "\n",
    "你可以在下方找到策略改进的完整伪代码。\n",
    "\n",
    "<div align=center><img width=\"550\" src=resource/5.png></div>\n",
    "\n",
    "如果对于某个状态$s\\in\\mathcal{S}$，其 $\\arg\\max_{a\\in\\mathcal{A}(s)}Q(s,a)$不是唯一的，则可以灵活地构建改进策略 $\\pi'$。\n",
    "\n",
    "实际上，只要策略 $\\pi'$对于每个 $s\\in\\mathcal{S}$ 和 $a\\in\\mathcal{A}(s) $都满足：\n",
    "\n",
    "$\\pi'(a|s) = 0 如果 a \\notin \\arg\\max_{a'\\in\\mathcal{A}(s)}Q(s,a')$，\n",
    "\n",
    "**它是改进策略。换句话说，（对于每个状态）任何策略只要为不会最大化动作值函数估值的动作分配概率 0，则对该状态来说就是改进策略。你可以在自己的实现中随意实验这一点！**\n",
    "\n",
    "### **实现：策略迭代**\n",
    "- 策略迭代是一种可以在动态规划设置中解决 MDP 的算法。它包含一系列的策略评估和改进步骤，肯定会收敛于最优策略（对应任意_有限_ MDP）\n",
    "\n",
    "‘在上一部分，你学习了策略迭代，即一系列的策略评估和改进步骤。策略迭代肯定会用有限次数的迭代找到任何有限马尔可夫决策流程 (MDP) 的最优策略。你可以在下方找到伪代码。'\n",
    "\n",
    "<div align=center><img width=\"550\" src=resource/6.png></div>\n",
    "\n",
    "### **实现：截断策略迭代**\n",
    "- 截断策略迭代是在动态规划设置中用来估算策略 $\\pi$ 对应的状态值函数 $v_\\pi$的算法。对于此方法，在对状态空间执行固定次数的遍历后，停止评估步骤。我们将评估步骤中的此方法称为截断策略评估\n",
    "\n",
    "在上个部分，你学习了截断策略迭代。（迭代性）策略评估会根据需要应用很多次贝尔曼更新步骤，以实现收敛，而截断策略迭代仅对整个状态空间执行固定次数的评估。\n",
    "**NB**:在评估中可以使用action value 计算，因为state value 和 action value是相对的。\n",
    "\n",
    "你可以在下方找到伪代码。\n",
    "<div align=center><img width=\"550\" src=resource/7.png></div>\n",
    "我们可以将这个修订后的策略评估算法应用到类似于策略评估的算法中，称之为截断策略迭代。\n",
    "\n",
    "你可以在下方找到伪代码\n",
    "<div align=center><img width=\"550\" src=resource/8.png></div>\n",
    "你可能还注意到，截断策略迭代的停止条件与策略迭代的不同。在策略迭代中，当策略在一次策略改进步骤之后没有变化时，我们将终止循环。在截断策略迭代中，仅当值函数估值收敛时，我们才停止循环\n",
    "\n",
    "强烈建议你尝试两种停止条件，以便熟练掌握这些知识。但是，我们发现如果超参数 `max_iterations` 设得太小，则无法检查策略是否更改了。（为此，思考下以下情形：假设 `max_iterations` 设为很小的值。如果算法完全没有收敛于最优值函数 $v_*$或最优策略 $\\pi_*$ ，可以想象，对值函数估值 V 的更新可能会太小，导致相应的策略没有任何更新。）\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **实现：值迭代**\n",
    "- 值迭代是在动态规划设置中用来估算策略$ \\pi$ 对应的状态值函数 $v_\\p$的算法。对于此方法，每次对状态空间进行遍历时，都同时进行策略评估和策略改进\n",
    "\n",
    "在上个部分，你学习了值迭代。在此算法中，对状态空间的每次遍历都会进行策略评估和策略改进。值迭代肯定会找到任何有限 MDP 的最优策略 $\\pi_*$ 。\n",
    "\n",
    "你可以在下方找到伪代码。\n",
    "\n",
    "<div align=center><img width=\"550\" src=resource/9.png></div>\n",
    "\n",
    "注意，如果后续值函数估值之间的差值很小，则满足了停止条件。尤其是，如果对于每个状态，差值都小于 $\\theta$，则循环终止。并且，如果我们希望最终值函数估值与最优值函数越接近，则需要将值 $\\theta$ 设得越小。\n",
    "\n",
    "你可以在你的实现中将 $\\theta$ 设成各种值；注意，对于 FrozenLake 环境，`1e-8` 左右的值似乎很合适。\n",
    "\n",
    "如果你想详细了解如何设置 $\\theta$ 的值，建议阅读[这篇论文](http://www.leemon.com/papers/1993wb2.pdf)，并重点看看 Theorem 3.2。他们的主要结果可以总结如下：\n",
    "\n",
    ">用$ V^{\\text{final}}$表示算法计算的最终值函数估值。然后可以发现 $V^{\\text{final}}$与最优值函数 $v_*$之间的差值最大为 $\\frac{2\\theta\\gamma}{1-\\gamma}$。换句话说，对于每个 $s\\in\\mathcal{S}$，\n",
    "\n",
    "\n",
    ">$\\max_{s\\in\\mathcal{S}}|V^{\\text{final}}(s) - v_*(s)| < \\frac{2\\theta\\gamma}{1-\\gamma}$ 。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
