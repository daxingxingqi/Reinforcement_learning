## 1. 问题描述


![img.png](default.png)

在该项目中，你将使用强化学习算法，实现一个自动走迷宫机器人。

1. 如上图所示，智能机器人显示在右上角。在我们的迷宫中，有陷阱（红色炸弹）及终点（蓝色的目标点）两种情景。机器人要尽量避开陷阱、尽快到达目的地。
2. 机器人可执行的动作包括：向上走 `u`、向右走 `r`、向下走 `d`、向左走 `l`。
3. 执行不同的动作后，根据不同的情况会获得不同的奖励，具体而言，有以下几种情况。
    - 撞到墙壁：-10
    - 走到终点：50
    - 走到陷阱：-30
    - 其余情况：-0.1
4. 我们需要通过修改 `robot.py` 中的代码，来实现一个 Q Learning 机器人，实现上述的目标。

## 2. 完成项目流程

1. 配置环境，使用 `envirnment.yml` 文件配置名为 `robot-env` 的 conda 环境，具体而言，你只需转到当前的目录，在命令行/终端中运行如下代码，稍作等待即可。
```
conda env create -f environment.yml
```
安装完毕后，在命令行/终端中运行 `source activate robot-env`（Mac/Linux 系统）或 `activate robot-env`（Windows 系统）激活该环境。

2. 阅读 `robot_maze.ipynb` 中的指导完成项目，并根据指导修改对应的代码，生成、观察结果。
3. 导出代码与报告，上传文件，提交审阅并优化。

## 3. 项目review

问题 1：请参照如上的定义，描述出 “机器人走迷宫这个问题” 中强化学习四个组成部分对应的实际对象。
- 【状态 : 墙壁，终点，陷阱及其它情况】对状态(State)的理解有些不到位。
  - 实际上状态就是描述机器人当前情况的一个抽象概念，它与机器人能否成功学到一个策略息息相关。
  - 在我们的项目中，它是小车所处的迷宫坐标位置，例如 (0,1)、(1,1) 等。
  - 如果将其设定为“处于起点”、“处于陷阱”、“处于终点”等情况，那么思考一下：
    - 这样设置与项目中设计的区别在哪里？
     > 如果按照”墙壁，终点，陷阱及其它情况“来设置状态的话，这些信息将不能作为抽象的信息被智能体所学习。
    - 这样机器人能够学习到一个成功的策略吗？
     > 不能，因为无法作为抽象信息被计算机所学习。

问题 2：根据已知条件求 $q(s_{t},a)$，在如下模板代码中的空格填入对应的数字即可
- R_{t+1} 是小车在S1执行动作u之后得到的奖励，不是下S2执行动作R得到的奖励。
- 这里的下标为t+1的含义是，在t的时刻执行动作a，则会在第t+1时刻得到对应的奖励 R_{t+1}。这里是 小车在 S1 执行了动作u之后，因为没有撞到墙壁，所以获得-0.1的奖励，见报告中，Section0的第一节的第三点。
- 相应地，max_a q(a,s_{t+1}) 代表着，在 t+1 时刻，在状态 s_{t+1} 下， 对所有的动作而言，小车 q 值能够取得的最大值，也就是我们在S2下的最大的Q值。

问题 4：在如下的代码块中，创建你的迷宫并展示。
- 你可以尝试使用 test_world 目录下的迷宫，或者修改 Maze 类中 __generate_maze 函数中的 complexity 及 density 变量，来增加迷宫的难度～

问题 6：实现 Robot.py 中的8段代码，并运行如下代码检查效果。
- 请根据下方的解释，优化一下你对 Q Learning 公式的理解：
- 在如下所示 Q-learning 更新公式中，它考虑了两部分的信息：之前学习到的Q值，以及新学习到Q值。

<div align=center><img width="550" src=qlearning.svg></div>
- 在新学习到的Q值中，γ*maxQ 的一项目就考虑了所谓的「未来奖励」——这是强化学习中的一个巨大亮点。也就是说，我们在计算、衡量一个动作的时候，不仅考虑它当前一步获得的奖励 r，还要考虑它执行这个动作之后带来的累计奖励——这能够帮助我们更好地衡量一个动作的好坏。但是这时候机器人并没有真正地往前走，而是使用Qtable 中原有地 next_state 的值来估计这个未来奖励。
- 其中 γ 是折扣因子，是一个(0,1)之间的值。一般我们取0.9，能够充分地对外来奖励进行考虑。如果这个值大于1，那么实际上未来奖励会发散开来（因为这是一个不断累加、迭代的过程），导致Qtable不能发散。它能够帮助终点处的正奖励“扩散”到周围，也就是说，这样机器人更能够成功地学习到通往终点的路径。

*问题 7：尝试利用下列代码训练机器人，并进行调参。*

【解释1】
- 首先给你补充一下对于 epsilon greedy 算法的解释：
- 对于 epsilon-greedy 算法，你可以参考论坛中的 这个帖子https://discussions.youdaxue.com/t/topic/33333：
> Q: 如何理解 greed-epsilon 方法／如何设置 epsilon／如何理解 exploration & exploitation 权衡？
A: (1) 我们的小车一开始接触到的 state 很少，并且如果小车按照已经学到的 qtable 执行，那么小车很有可能出错或者绕圈圈。同时我们希望小车一开始能随机的走一走，接触到更多的 state。(2) 基于上述原因，我们希望小车在一开始的时候不完全按照 Q learning 的结果运行，即以一定的概率 epsilon，随机选择 action，而不是根据 maxQ 来选择 action。然后随着不断的学习，那么我会降低这个随机的概率，使用一个衰减函数来降低 epsilon。(3) 这个就解决了所谓的 exploration and exploitation 的问题，在“探索”和“执行”之间寻找一个权衡。

【解释2】
- 再给你补充一下对 alpha 的解释。 alpha 是一个权衡上一次学到结果和这一次学习结果的量，如：Q = (1-alpha)*Q_old + alpha*Q_current。
- alpha 设置过低会导致机器人只在乎之前的知识，而不能积累新的 reward。一般取 0.5 来均衡以前知识及新的 reward。

【解释3】
- gamma 是考虑未来奖励的因子，是一个(0,1)之间的值。一般我们取0.9，能够充分地对外来奖励进行考虑。
- 实际上如果你将它调小了，你会发现终点处的正奖励不能够“扩散”到周围，也就是说，机器人很有可能无法学习到一个到达终点的策略。你可以自己尝试一下。

问题 8：使用 runner.plot_results() 输出训练结果，根据该结果对你的机器人进行分析。

-请进一步补充你的分析：
  - 我们希望你在这里能够更详细地说明每个参数（alpha、gamma、epsilon0 和 epsilon下降函数两者的区别和联系、训练次数）的作用是什么，它们的变化大概会怎样影响运行结果，然后有目的地对小车进行调参，比较不同参数下的训练结果，并说明你使用这个参数的原因。
  - 总结出这些参数值的变化将如何影响你小车的训练结果。
  - 对比在不同的参数组合下小车的运行结果，并将结果打印出来你（你可以复制 runner.plot_results() 代码对结果多次打印。请至少对比三组参数组合的结果。
- 这样你的报告会更加严谨～

- 在训练中，不同的迷宫将会给训练带来很大的影响，如：
  - 迷宫本身较大或者较难，不利于训练，将会造成训练结果很糟糕。
  - 陷阱位置不好：如下图所示，在通往终点的路上，有一个陷阱，也会导致训练结果变差。

- 因此，建议你固定迷宫，再对比不同参数下的结果；同时你也可以把迷宫看作一个可以修改的变量，来看看如何调整参数，才能来对复杂迷宫学到一个成功策略。

- 关于炸弹堵死道路的问题，你是不是可以：
  - 设置机器人，在遇到陷阱的情况下，增大随机探索的机率，能从陷阱跳出来？
  - 尝试调整 reward 的设置？
